
Code for root model, type=ScriptableAdapter:
class ScriptableAdapter(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  model : __torch__.detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN
  training : Final[bool] = False
  def forward(self: __torch__.ScriptableAdapter,
    inputs: Tuple[Dict[str, Tensor]]) -> List[Dict[str, Tensor]]:
    _0, = inputs
    instances = (self.model).inference([_0], None, False, )
    _1 = annotate(List[Dict[str, Tensor]], [])
    for _2 in range(torch.len(instances)):
      i = instances[_2]
      _3 = torch.append(_1, (i).get_fields())
    return _1

--------------------------------------------------------------------------------
Code for .model, type=GeneralizedRCNN:
class GeneralizedRCNN(Module):
  __parameters__ = []
  __buffers__ = ["pixel_mean", "pixel_std", ]
  pixel_mean : Tensor
  pixel_std : Tensor
  _is_full_backward_hook : Optional[bool]
  input_format : str
  vis_period : int
  backbone : __torch__.detectron2.modeling.backbone.fpn.FPN
  proposal_generator : __torch__.detectron2.modeling.proposal_generator.rpn.RPN
  roi_heads : __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads
  training : Final[bool] = False
  def __device_getter(self: __torch__.detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN) -> Device:
    return ops.prim.device(self.pixel_mean)
  def forward(self: __torch__.detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN,
    batched_inputs: List[Dict[str, Tensor]]) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    _0 = (self).inference(batched_inputs, None, True, )
    return _0
  def inference(self: __torch__.detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN,
    batched_inputs: List[Dict[str, Tensor]],
    detected_instances: Optional[List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]]=None,
    do_postprocess: bool=True) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    _1 = "AssertionError: Scripting is not supported for postprocess."
    _2 = uninitialized(List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1])
    images = (self).preprocess_image(batched_inputs, )
    features = (self.backbone).forward(images.tensor, )
    _3 = torch.__is__(detected_instances, None)
    if _3:
      _4 = (self.proposal_generator).forward(images, features, None, )
      proposals, _5, = _4
      _6 = (self.roi_heads).forward(images, features, proposals, None, )
      results0, _7, = _6
      results = results0
    else:
      detected_instances0 = unchecked_cast(List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1], detected_instances)
      detected_instances1 = annotate(List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1], [])
      for _8 in range(torch.len(detected_instances0)):
        x = detected_instances0[_8]
        _9 = (x).to((self).__device_getter(), )
        _10 = torch.append(detected_instances1, _9)
      results1 = (self.roi_heads).forward_with_given_boxes(features, detected_instances1, )
      results = results1
    if do_postprocess:
      ops.prim.RaiseException(_1)
      _11 = _2
    else:
      _11 = results
    return _11
  def preprocess_image(self: __torch__.detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN,
    batched_inputs: List[Dict[str, Tensor]]) -> __torch__.detectron2.structures.image_list.ImageList:
    _12 = __torch__.detectron2.structures.image_list.from_tensors
    images = annotate(List[Tensor], [])
    for _13 in range(torch.len(batched_inputs)):
      x = batched_inputs[_13]
      _14 = torch.to(x["image"], (self).__device_getter(), None, False, False)
      _15 = torch.append(images, _14)
    images0 = annotate(List[Tensor], [])
    for _16 in range(torch.len(images)):
      x0 = images[_16]
      _17 = torch.sub(x0, self.pixel_mean, alpha=1)
      _18 = torch.append(images0, torch.div(_17, self.pixel_std))
    _19 = (self.backbone).__size_divisibility_getter()
    return _12(images0, _19, 0., )

--------------------------------------------------------------------------------
Code for .model.backbone, type=FPN:
class FPN(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : None
  in_features : Tuple[str, str, str, str]
  _out_feature_strides : Dict[str, int]
  _out_features : List[str]
  _out_feature_channels : Dict[str, int]
  _size_divisibility : int
  top_block : __torch__.detectron2.modeling.backbone.fpn.LastLevelMaxPool
  bottom_up : __torch__.detectron2.modeling.backbone.resnet.ResNet
  lateral_convs : __torch__.torch.nn.modules.container.___torch_mangle_33.ModuleList
  output_convs : __torch__.torch.nn.modules.container.___torch_mangle_35.ModuleList
  _fuse_type : Final[str] = "sum"
  training : Final[bool] = False
  def __size_divisibility_getter(self: __torch__.detectron2.modeling.backbone.fpn.FPN) -> int:
    return self._size_divisibility
  def forward(self: __torch__.detectron2.modeling.backbone.fpn.FPN,
    x: Tensor) -> Dict[str, Tensor]:
    _0 = __torch__.torch.nn.functional.___torch_mangle_43.interpolate
    bottom_up_features = (self.bottom_up).forward(x, )
    results = annotate(List[Tensor], [])
    _1 = getattr(self.lateral_convs, "0")
    _2 = bottom_up_features[(self.in_features)[-1]]
    prev_features = (_1).forward(_2, )
    _3 = (getattr(self.output_convs, "0")).forward(prev_features, )
    _4 = torch.append(results, _3)
    _5 = self.lateral_convs
    _6 = getattr(_5, "1")
    _7 = getattr(_5, "2")
    _8 = getattr(_5, "3")
    _9 = self.output_convs
    _10 = getattr(_9, "1")
    _11 = getattr(_9, "2")
    _12 = getattr(_9, "3")
    features = (self.in_features)[-2]
    features0 = bottom_up_features[features]
    top_down_features = _0(prev_features, None, 2., "nearest", None, None, )
    lateral_features = (_6).forward(features0, )
    prev_features0 = torch.add(lateral_features, top_down_features, alpha=1)
    torch.insert(results, 0, (_10).forward(prev_features0, ))
    features1 = (self.in_features)[-3]
    features2 = bottom_up_features[features1]
    top_down_features0 = _0(prev_features0, None, 2., "nearest", None, None, )
    lateral_features0 = (_7).forward(features2, )
    prev_features1 = torch.add(lateral_features0, top_down_features0, alpha=1)
    torch.insert(results, 0, (_11).forward(prev_features1, ))
    features3 = (self.in_features)[-4]
    features4 = bottom_up_features[features3]
    top_down_features1 = _0(prev_features1, None, 2., "nearest", None, None, )
    lateral_features1 = (_8).forward(features4, )
    prev_features2 = torch.add(lateral_features1, top_down_features1, alpha=1)
    torch.insert(results, 0, (_12).forward(prev_features2, ))
    _13 = torch.__contains__(bottom_up_features, self.top_block.in_feature)
    if _13:
      top_block_in_feature0 = bottom_up_features[self.top_block.in_feature]
      top_block_in_feature = top_block_in_feature0
    else:
      _14 = torch.index(self._out_features, self.top_block.in_feature)
      top_block_in_feature = results[_14]
    _15 = (self.top_block).forward(top_block_in_feature, )
    torch.extend(results, _15)
    _16 = torch.eq(torch.len(self._out_features), torch.len(results))
    if _16:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    _17 = annotate(Dict[str, Tensor], {})
    _18 = self._out_features
    _19 = [torch.len(_18), torch.len(results)]
    for _20 in range(ops.prim.min(_19)):
      f = _18[_20]
      res = results[_20]
      torch._set_item(_17, f, res)
    return _17

--------------------------------------------------------------------------------
Code for .model.backbone.top_block, type=LastLevelMaxPool:
class LastLevelMaxPool(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  num_levels : int
  in_feature : str
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.fpn.LastLevelMaxPool,
    x: Tensor) -> List[Tensor]:
    _0 = __torch__.torch.nn.functional._max_pool2d
    _1 = _0(x, [1, 1], [2, 2], [0, 0], [1, 1], False, False, )
    return [_1]

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up, type=ResNet:
class ResNet(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  num_classes : None
  _out_feature_strides : Dict[str, int]
  _out_feature_channels : Dict[str, int]
  stage_names : Tuple[str, str, str, str]
  _out_features : List[str]
  stem : __torch__.detectron2.modeling.backbone.resnet.BasicStem
  stages : __torch__.torch.nn.modules.container.ModuleList
  training : Final[bool] = False
  def __size_divisibility_getter(self: __torch__.detectron2.modeling.backbone.resnet.ResNet) -> int:
    return 0
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.ResNet,
    x: Tensor) -> Dict[str, Tensor]:
    _0 = "ResNet takes an input of shape (N, C, H, W). Got {} instead!"
    if torch.eq(torch.dim(x), 4):
      pass
    else:
      _1 = torch.add("AssertionError: ", torch.format(_0, torch.size(x)))
      ops.prim.RaiseException(_1)
    outputs = annotate(Dict[str, Tensor], {})
    x0 = (self.stem).forward(x, )
    _2 = torch.__contains__(self._out_features, "stem")
    if _2:
      torch._set_item(outputs, "stem", x0)
    else:
      pass
    name, name0, name1, name2, = self.stage_names
    _3 = self.stages
    _4 = getattr(_3, "0")
    _5 = getattr(_3, "1")
    _6 = getattr(_3, "2")
    _7 = getattr(_3, "3")
    x1 = (_4).forward(x0, )
    _8 = torch.__contains__(self._out_features, name)
    if _8:
      torch._set_item(outputs, name, x1)
    else:
      pass
    x2 = (_5).forward(x1, )
    _9 = torch.__contains__(self._out_features, name0)
    if _9:
      torch._set_item(outputs, name0, x2)
    else:
      pass
    x3 = (_6).forward(x2, )
    _10 = torch.__contains__(self._out_features, name1)
    if _10:
      torch._set_item(outputs, name1, x3)
    else:
      pass
    x4 = (_7).forward(x3, )
    _11 = torch.__contains__(self._out_features, name2)
    if _11:
      torch._set_item(outputs, name2, x4)
    else:
      pass
    return outputs

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem, type=BasicStem:
class BasicStem(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  conv1 : __torch__.detectron2.layers.wrappers.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.BasicStem,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional._max_pool2d
    x0 = (self.conv1).forward(x, )
    x1 = torch.relu_(x0)
    x2 = _0(x1, [3, 3], [2, 2], [1, 1], [1, 1], False, False, )
    return x2

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 64
  in_channels : Final[int] = 3
  kernel_size : Final[Tuple[int, int]] = (7, 7)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (3, 3)
  stride : Final[Tuple[int, int]] = (2, 2)
  def forward(self: __torch__.detectron2.layers.wrappers.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [2, 2], [3, 3], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.torch.nn.modules.container.Sequential
  __annotations__["1"] = __torch__.torch.nn.modules.container.___torch_mangle_12.Sequential
  __annotations__["2"] = __torch__.torch.nn.modules.container.___torch_mangle_20.Sequential
  __annotations__["3"] = __torch__.torch.nn.modules.container.___torch_mangle_28.Sequential
  training : Final[bool] = True
  def forward(self: __torch__.torch.nn.modules.container.ModuleList) -> None:
    _0 = uninitialized(None)
    ops.prim.RaiseException("")
    return _0
  def __len__(self: __torch__.torch.nn.modules.container.ModuleList) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_4.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_4.BottleneckBlock
  training : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.container.Sequential,
    input: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    _1 = getattr(self, "1")
    _2 = getattr(self, "2")
    input0 = (_0).forward(input, )
    input1 = (_1).forward(input0, )
    return (_2).forward(input1, )
  def __len__(self: __torch__.torch.nn.modules.container.Sequential) -> int:
    return 3

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_1.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    shortcut = (self.shortcut).forward(x, )
    out4 = torch.add_(out3, shortcut, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 64
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_1.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 64
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_3.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_4.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 64
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_3.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 64
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_3.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_4.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 64
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_3.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 64
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_9.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_11.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_11.BottleneckBlock
  __annotations__["3"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_11.BottleneckBlock
  training : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_12.Sequential,
    input: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    _1 = getattr(self, "1")
    _2 = getattr(self, "2")
    _3 = getattr(self, "3")
    input0 = (_0).forward(input, )
    input1 = (_1).forward(input0, )
    input2 = (_2).forward(input1, )
    return (_3).forward(input2, )
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_12.Sequential) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_5.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_6.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_9.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    shortcut = (self.shortcut).forward(x, )
    out4 = torch.add_(out3, shortcut, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 512
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (2, 2)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_5.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [2, 2], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 128
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (2, 2)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_6.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [2, 2], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 128
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 512
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_10.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_11.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 128
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_10.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 128
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 512
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_10.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_11.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 128
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_10.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 128
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 512
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_10.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_11.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 128
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_10.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 128
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 512
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_8.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_17.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["3"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["4"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["5"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["6"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["7"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["8"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["9"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["10"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["11"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["12"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["13"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["14"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["15"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["16"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["17"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["18"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["19"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["20"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["21"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  __annotations__["22"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock
  training : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_20.Sequential,
    input: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    _1 = getattr(self, "1")
    _2 = getattr(self, "2")
    _3 = getattr(self, "3")
    _4 = getattr(self, "4")
    _5 = getattr(self, "5")
    _6 = getattr(self, "6")
    _7 = getattr(self, "7")
    _8 = getattr(self, "8")
    _9 = getattr(self, "9")
    _10 = getattr(self, "10")
    _11 = getattr(self, "11")
    _12 = getattr(self, "12")
    _13 = getattr(self, "13")
    _14 = getattr(self, "14")
    _15 = getattr(self, "15")
    _16 = getattr(self, "16")
    _17 = getattr(self, "17")
    _18 = getattr(self, "18")
    _19 = getattr(self, "19")
    _20 = getattr(self, "20")
    _21 = getattr(self, "21")
    _22 = getattr(self, "22")
    input0 = (_0).forward(input, )
    input1 = (_1).forward(input0, )
    input2 = (_2).forward(input1, )
    input3 = (_3).forward(input2, )
    input4 = (_4).forward(input3, )
    input5 = (_5).forward(input4, )
    input6 = (_6).forward(input5, )
    input7 = (_7).forward(input6, )
    input8 = (_8).forward(input7, )
    input9 = (_9).forward(input8, )
    input10 = (_10).forward(input9, )
    input11 = (_11).forward(input10, )
    input12 = (_12).forward(input11, )
    input13 = (_13).forward(input12, )
    input14 = (_14).forward(input13, )
    input15 = (_15).forward(input14, )
    input16 = (_16).forward(input15, )
    input17 = (_17).forward(input16, )
    input18 = (_18).forward(input17, )
    input19 = (_19).forward(input18, )
    input20 = (_20).forward(input19, )
    input21 = (_21).forward(input20, )
    return (_22).forward(input21, )
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_20.Sequential) -> int:
    return 23

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_13.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_14.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_17.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    shortcut = (self.shortcut).forward(x, )
    out4 = torch.add_(out3, shortcut, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (2, 2)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_13.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [2, 2], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (2, 2)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_14.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [2, 2], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.6, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.6.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.6.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.6.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.6.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.6.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.6.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.7, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.7.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.7.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.7.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.7.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.7.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.7.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.8, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.8.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.8.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.8.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.8.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.8.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.8.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.9, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.9.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.9.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.9.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.9.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.9.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.9.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.10, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.10.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.10.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.10.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.10.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.10.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.10.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.11, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.11.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.11.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.11.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.11.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.11.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.11.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.12, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.12.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.12.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.12.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.12.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.12.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.12.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.13, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.13.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.13.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.13.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.13.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.13.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.13.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.14, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.14.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.14.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.14.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.14.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.14.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.14.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.15, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.15.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.15.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.15.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.15.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.15.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.15.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.16, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.16.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.16.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.16.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.16.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.16.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.16.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.17, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.17.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.17.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.17.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.17.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.17.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.17.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.18, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.18.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.18.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.18.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.18.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.18.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.18.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.19, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.19.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.19.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.19.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.19.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.19.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.19.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.20, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.20.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.20.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.20.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.20.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.20.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.20.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.21, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.21.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.21.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.21.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.21.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.21.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.21.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.22, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_19.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.22.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_18.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.22.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.22.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.22.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.22.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 1024
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_16.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.22.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_25.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_27.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_27.BottleneckBlock
  training : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_28.Sequential,
    input: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    _1 = getattr(self, "1")
    _2 = getattr(self, "2")
    input0 = (_0).forward(input, )
    input1 = (_1).forward(input0, )
    return (_2).forward(input1, )
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_28.Sequential) -> int:
    return 3

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_21.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_22.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_23.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_25.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    shortcut = (self.shortcut).forward(x, )
    out4 = torch.add_(out3, shortcut, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 2048
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (2, 2)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_21.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [2, 2], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 512
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (2, 2)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_22.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [2, 2], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 512
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_23.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 2048
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_26.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_23.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_27.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 512
  in_channels : Final[int] = 2048
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_26.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 512
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_23.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 2048
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : None
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_26.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_23.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_27.BottleneckBlock,
    x: Tensor) -> Tensor:
    out = (self.conv1).forward(x, )
    out0 = torch.relu_(out)
    out1 = (self.conv2).forward(out0, )
    out2 = torch.relu_(out1)
    out3 = (self.conv3).forward(out2, )
    out4 = torch.add_(out3, x, alpha=1)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 512
  in_channels : Final[int] = 2048
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_26.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 512
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_23.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  activation : None
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  out_channels : Final[int] = 2048
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return (self.norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.batch_norm
    if ops.prim.requires_grad(x):
      _2 = self.weight
      _3 = torch.add(self.running_var, self.eps, 1)
      scale = torch.mul(_2, torch.rsqrt(_3))
      _4 = self.bias
      _5 = torch.mul(self.running_mean, scale)
      bias = torch.sub(_4, _5, alpha=1)
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias0 = torch.reshape(bias, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _6 = torch.to(scale0, out_dtype, False, False, None)
      _7 = torch.mul(x, _6)
      _8 = torch.to(bias0, out_dtype, False, False, None)
      _1 = torch.add(_7, _8, alpha=1)
    else:
      _9 = _0(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0.10000000000000001, self.eps, )
      _1 = _9
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.layers.wrappers.___torch_mangle_29.Conv2d
  __annotations__["1"] = __torch__.detectron2.layers.wrappers.___torch_mangle_30.Conv2d
  __annotations__["2"] = __torch__.detectron2.layers.wrappers.___torch_mangle_31.Conv2d
  __annotations__["3"] = __torch__.detectron2.layers.wrappers.___torch_mangle_32.Conv2d
  training : Final[bool] = True
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_33.ModuleList) -> None:
    _0 = uninitialized(None)
    ops.prim.RaiseException("")
    return _0
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_33.ModuleList) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs.0, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  norm : None
  activation : None
  out_channels : Final[int] = 256
  in_channels : Final[int] = 2048
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_29.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  norm : None
  activation : None
  out_channels : Final[int] = 256
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_30.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs.2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  norm : None
  activation : None
  out_channels : Final[int] = 256
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_31.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs.3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  norm : None
  activation : None
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_32.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [0, 0], [1, 1], 1)
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d
  __annotations__["1"] = __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d
  __annotations__["2"] = __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d
  __annotations__["3"] = __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d
  training : Final[bool] = True
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_35.ModuleList) -> None:
    _0 = uninitialized(None)
    ops.prim.RaiseException("")
    return _0
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_35.ModuleList) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs.0, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  norm : None
  activation : None
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  norm : None
  activation : None
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs.2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  norm : None
  activation : None
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs.3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  norm : None
  activation : None
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_34.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return x0

--------------------------------------------------------------------------------
Code for .model.proposal_generator, type=RPN:
class RPN(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_features : List[str]
  box2box_transform : __torch__.detectron2.modeling.box_regression.Box2BoxTransform
  batch_size_per_image : int
  positive_fraction : float
  pre_nms_topk : Dict[bool, int]
  post_nms_topk : Dict[bool, int]
  nms_thresh : float
  min_box_size : float
  anchor_boundary_thresh : int
  loss_weight : Dict[str, float]
  box_reg_loss_type : str
  smooth_l1_beta : float
  rpn_head : __torch__.detectron2.modeling.proposal_generator.rpn.StandardRPNHead
  anchor_generator : __torch__.detectron2.modeling.anchor_generator.DefaultAnchorGenerator
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.proposal_generator.rpn.RPN,
    images: __torch__.detectron2.structures.image_list.ImageList,
    features: Dict[str, Tensor],
    gt_instances: Optional[List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]]=None) -> Tuple[List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1], Dict[str, Tensor]]:
    features0 = annotate(List[Tensor], [])
    _0 = self.in_features
    for _1 in range(torch.len(_0)):
      f = _0[_1]
      _2 = torch.append(features0, features[f])
    anchors = (self.anchor_generator).forward(features0, )
    _3 = (self.rpn_head).forward(features0, )
    pred_objectness_logits, pred_anchor_deltas, = _3
    pred_objectness_logits0 = annotate(List[Tensor], [])
    for _4 in range(torch.len(pred_objectness_logits)):
      score = pred_objectness_logits[_4]
      _5 = torch.permute(score, [0, 2, 3, 1])
      _6 = torch.append(pred_objectness_logits0, torch.flatten(_5, 1, -1))
    pred_anchor_deltas0 = annotate(List[Tensor], [])
    for _7 in range(torch.len(pred_anchor_deltas)):
      x = pred_anchor_deltas[_7]
      _8 = [(torch.size(x))[0], -1, 4, (torch.size(x))[-2], (torch.size(x))[-1]]
      _9 = torch.permute(torch.view(x, _8), [0, 3, 4, 1, 2])
      _10 = torch.append(pred_anchor_deltas0, torch.flatten(_9, 1, -2))
    losses = annotate(Dict[str, Tensor], {})
    proposals = (self).predict_proposals(anchors, pred_objectness_logits0, pred_anchor_deltas0, images.image_sizes, )
    return (proposals, losses)
  def predict_proposals(self: __torch__.detectron2.modeling.proposal_generator.rpn.RPN,
    anchors: List[__torch__.detectron2.structures.boxes.Boxes],
    pred_objectness_logits: List[Tensor],
    pred_anchor_deltas: List[Tensor],
    image_sizes: List[Tuple[int, int]]) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    _11 = __torch__.detectron2.modeling.proposal_generator.proposal_utils.find_top_rpn_proposals
    _12 = __torch__.torch.autograd.grad_mode.no_grad.__new__(__torch__.torch.autograd.grad_mode.no_grad)
    _13 = (_12).__init__()
    with _12:
      pred_proposals = (self)._decode_proposals(anchors, pred_anchor_deltas, )
      _14 = _11(pred_proposals, pred_objectness_logits, image_sizes, self.nms_thresh, (self.pre_nms_topk)[False], (self.post_nms_topk)[False], self.min_box_size, False, )
    return _14
  def _decode_proposals(self: __torch__.detectron2.modeling.proposal_generator.rpn.RPN,
    anchors: List[__torch__.detectron2.structures.boxes.Boxes],
    pred_anchor_deltas: List[Tensor]) -> List[Tensor]:
    N = (torch.size(pred_anchor_deltas[0]))[0]
    proposals = annotate(List[Tensor], [])
    _15 = [torch.len(anchors), torch.len(pred_anchor_deltas)]
    for _16 in range(ops.prim.min(_15)):
      anchors_i = anchors[_16]
      pred_anchor_deltas_i = pred_anchor_deltas[_16]
      B = torch.size(anchors_i.tensor, 1)
      pred_anchor_deltas_i0 = torch.reshape(pred_anchor_deltas_i, [-1, B])
      _17 = torch.unsqueeze(anchors_i.tensor, 0)
      _18 = torch.expand(_17, [N, -1, -1], implicit=False)
      anchors_i0 = torch.reshape(_18, [-1, B])
      proposals_i = (self.box2box_transform).apply_deltas(pred_anchor_deltas_i0, anchors_i0, )
      _19 = torch.view(proposals_i, [N, -1, B])
      _20 = torch.append(proposals, _19)
    return proposals

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head, type=StandardRPNHead:
class StandardRPNHead(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.detectron2.layers.wrappers.___torch_mangle_36.Conv2d
  objectness_logits : __torch__.torch.nn.modules.conv.Conv2d
  anchor_deltas : __torch__.torch.nn.modules.conv.___torch_mangle_37.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.proposal_generator.rpn.StandardRPNHead,
    features: List[Tensor]) -> Tuple[List[Tensor], List[Tensor]]:
    pred_objectness_logits = annotate(List[Tensor], [])
    pred_anchor_deltas = annotate(List[Tensor], [])
    for _0 in range(torch.len(features)):
      x = features[_0]
      t = (self.conv).forward(x, )
      _1 = (self.objectness_logits).forward(t, )
      _2 = torch.append(pred_objectness_logits, _1)
      _3 = torch.append(pred_anchor_deltas, (self.anchor_deltas).forward(t, ))
    _4 = (pred_objectness_logits, pred_anchor_deltas)
    return _4

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  norm : None
  activation : __torch__.torch.nn.modules.activation.ReLU
  out_channels : Final[int] = 256
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (1, 1)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_36.Conv2d,
    x: Tensor) -> Tensor:
    x0 = torch.conv2d(x, self.weight, self.bias, [1, 1], [1, 1], [1, 1], 1)
    return (self.activation).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.conv.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : None
  inplace : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.relu(input, False, )
    return _0

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.objectness_logits, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  out_channels : Final[int] = 3
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.torch.nn.modules.conv.Conv2d,
    input: Tensor) -> Tensor:
    _0 = (self)._conv_forward(input, self.weight, self.bias, )
    return _0
  def _conv_forward(self: __torch__.torch.nn.modules.conv.Conv2d,
    input: Tensor,
    weight: Tensor,
    bias: Optional[Tensor]) -> Tensor:
    _1 = torch.conv2d(input, weight, bias, [1, 1], [0, 0], [1, 1], 1)
    return _1

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.anchor_deltas, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : None
  transposed : bool
  _reversed_padding_repeated_twice : Tuple[int, int, int, int]
  out_channels : Final[int] = 12
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  padding_mode : Final[str] = "zeros"
  groups : Final[int] = 1
  dilation : Final[Tuple[int, int]] = (1, 1)
  padding : Final[Tuple[int, int]] = (0, 0)
  stride : Final[Tuple[int, int]] = (1, 1)
  def forward(self: __torch__.torch.nn.modules.conv.___torch_mangle_37.Conv2d,
    input: Tensor) -> Tensor:
    _0 = (self)._conv_forward(input, self.weight, self.bias, )
    return _0
  def _conv_forward(self: __torch__.torch.nn.modules.conv.___torch_mangle_37.Conv2d,
    input: Tensor,
    weight: Tensor,
    bias: Optional[Tensor]) -> Tensor:
    _1 = torch.conv2d(input, weight, bias, [1, 1], [0, 0], [1, 1], 1)
    return _1

--------------------------------------------------------------------------------
Code for .model.proposal_generator.anchor_generator, type=DefaultAnchorGenerator:
class DefaultAnchorGenerator(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : None
  strides : List[int]
  num_features : int
  offset : float
  cell_anchors : __torch__.detectron2.modeling.anchor_generator.BufferList
  box_dim : Final[int] = 4
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.anchor_generator.DefaultAnchorGenerator,
    features: List[Tensor]) -> List[__torch__.detectron2.structures.boxes.Boxes]:
    grid_sizes = annotate(List[List[int]], [])
    for _0 in range(torch.len(features)):
      feature_map = features[_0]
      _1 = torch.slice(torch.size(feature_map), -2, 9223372036854775807, 1)
      _2 = torch.append(grid_sizes, _1)
    anchors_over_all_feature_maps = (self)._grid_anchors(grid_sizes, )
    _3 = annotate(List[__torch__.detectron2.structures.boxes.Boxes], [])
    _4 = torch.len(anchors_over_all_feature_maps)
    for _5 in range(_4):
      x = anchors_over_all_feature_maps[_5]
      _6 = __torch__.detectron2.structures.boxes.Boxes.__new__(__torch__.detectron2.structures.boxes.Boxes)
      _7 = (_6).__init__(x, )
      _8 = torch.append(_3, _6)
    return _3
  def _grid_anchors(self: __torch__.detectron2.modeling.anchor_generator.DefaultAnchorGenerator,
    grid_sizes: List[List[int]]) -> List[Tensor]:
    _9 = __torch__.detectron2.modeling.anchor_generator._create_grid_offsets
    anchors = annotate(List[Tensor], [])
    buffers = annotate(List[Tensor], [])
    _10 = self.cell_anchors
    _11 = getattr(_10, "0")
    _12 = getattr(_10, "1")
    _13 = getattr(_10, "2")
    _14 = getattr(_10, "3")
    _15 = getattr(_10, "4")
    _16 = torch.append(buffers, _11)
    _17 = torch.append(buffers, _12)
    _18 = torch.append(buffers, _13)
    _19 = torch.append(buffers, _14)
    _20 = torch.append(buffers, _15)
    _21 = self.strides
    _22 = [torch.len(grid_sizes), torch.len(_21), torch.len(buffers)]
    for _23 in range(ops.prim.min(_22)):
      size = grid_sizes[_23]
      stride = _21[_23]
      base_anchors = buffers[_23]
      _24 = _9(size, stride, self.offset, ops.prim.device(base_anchors), )
      shift_x, shift_y, = _24
      _25 = [shift_x, shift_y, shift_x, shift_y]
      shifts = torch.stack(_25, 1)
      _26 = torch.view(shifts, [-1, 1, 4])
      _27 = torch.view(base_anchors, [1, -1, 4])
      _28 = torch.reshape(torch.add(_26, _27, alpha=1), [-1, 4])
      _29 = torch.append(anchors, _28)
    return anchors

--------------------------------------------------------------------------------
Code for .model.proposal_generator.anchor_generator.cell_anchors, type=BufferList:
class BufferList(Module):
  __parameters__ = []
  __buffers__ = ["0", "1", "2", "3", "4", ]
  __annotations__ = []
  __annotations__["0"] = Tensor
  __annotations__["1"] = Tensor
  __annotations__["2"] = Tensor
  __annotations__["3"] = Tensor
  __annotations__["4"] = Tensor
  _is_full_backward_hook : Optional[bool]
  training : Final[bool] = False

--------------------------------------------------------------------------------
Code for .model.roi_heads, type=StandardROIHeads:
class StandardROIHeads(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  batch_size_per_image : int
  positive_fraction : float
  num_classes : int
  proposal_append_gt : bool
  in_features : List[str]
  box_in_features : List[str]
  train_on_pred_boxes : bool
  box_pooler : __torch__.detectron2.modeling.poolers.ROIPooler
  box_head : __torch__.detectron2.modeling.roi_heads.box_head.FastRCNNConvFCHead
  box_predictor : __torch__.detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers
  mask_on : Final[bool] = False
  keypoint_on : Final[bool] = False
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads,
    images: __torch__.detectron2.structures.image_list.ImageList,
    features: Dict[str, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1],
    targets: Optional[List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]]=None) -> Tuple[List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1], Dict[str, Tensor]]:
    pred_instances = (self)._forward_box(features, proposals, )
    pred_instances0 = (self).forward_with_given_boxes(features, pred_instances, )
    _0 = (pred_instances0, annotate(Dict[str, Tensor], {}))
    return _0
  def _forward_box(self: __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads,
    features: Dict[str, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    features0 = annotate(List[Tensor], [])
    _1 = self.box_in_features
    for _2 in range(torch.len(_1)):
      f = _1[_2]
      _3 = torch.append(features0, features[f])
    _4 = self.box_pooler
    _5 = annotate(List[__torch__.detectron2.structures.boxes.Boxes], [])
    for _6 in range(torch.len(proposals)):
      x = proposals[_6]
      _7 = torch.append(_5, (x).__proposal_boxes_getter())
    box_features = (_4).forward(features0, _5, )
    box_features0 = (self.box_head).forward(box_features, )
    predictions = (self.box_predictor).forward(box_features0, )
    _8 = (self.box_predictor).inference(predictions, proposals, )
    pred_instances, _9, = _8
    return pred_instances
  def forward_with_given_boxes(self: __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads,
    features: Dict[str, Tensor],
    instances: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    if (instances[0]).has("pred_boxes", ):
      _11 = (instances[0]).has("pred_classes", )
      _10 = _11
    else:
      _10 = False
    if _10:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    instances0 = (self)._forward_mask(features, instances, )
    instances1 = (self)._forward_keypoint(features, instances0, )
    return instances1
  def _forward_mask(self: __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads,
    features: Dict[str, Tensor],
    instances: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    return instances
  def _forward_keypoint(self: __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads,
    features: Dict[str, Tensor],
    instances: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]:
    return instances

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler, type=ROIPooler:
class ROIPooler(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  min_level : int
  max_level : int
  canonical_level : int
  canonical_box_size : int
  level_poolers : __torch__.torch.nn.modules.container.___torch_mangle_38.ModuleList
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.poolers.ROIPooler,
    x: List[Tensor],
    box_lists: List[__torch__.detectron2.structures.boxes.Boxes]) -> Tensor:
    _0 = "unequal value, num_level_assignments={}, but x is list of {} Tensors"
    _1 = "unequal value, x[0] batch dim 0 is {}, but box_list has length {}"
    _2 = __torch__.detectron2.modeling.poolers.convert_boxes_to_pooler_format
    _3 = __torch__.detectron2.modeling.poolers.assign_boxes_to_levels
    _4 = __torch__.detectron2.layers.wrappers.nonzero_tuple
    num_level_assignments = (self.level_poolers).__len__()
    _5 = torch.eq(torch.len(x), num_level_assignments)
    if _5:
      pass
    else:
      _6 = torch.format(_0, num_level_assignments, torch.len(x))
      ops.prim.RaiseException(torch.add("AssertionError: ", _6))
    _7 = torch.eq(torch.len(box_lists), torch.size(x[0], 0))
    if _7:
      pass
    else:
      _8 = torch.format(_1, torch.size(x[0], 0), torch.len(box_lists))
      ops.prim.RaiseException(torch.add("AssertionError: ", _8))
    if torch.eq(torch.len(box_lists), 0):
      _10 = (torch.size(x[0]))[1]
      _11 = self.output_size
      _12 = [0, _10]
      _13, _14, = _11
      _15 = torch.add(_12, [_13, _14])
      _16 = ops.prim.device(x[0])
      _17 = torch.zeros(_15, dtype=ops.prim.dtype(x[0]), layout=None, device=_16, pin_memory=None)
      _9 = _17
    else:
      pooler_fmt_boxes = _2(box_lists, )
      _18 = torch.eq(num_level_assignments, 1)
      if _18:
        _20 = getattr(self.level_poolers, "0")
        _21 = (_20).forward(x[0], pooler_fmt_boxes, )
        _19 = _21
      else:
        level_assignments = _3(box_lists, self.min_level, self.max_level, self.canonical_box_size, self.canonical_level, )
        num_boxes = torch.size(pooler_fmt_boxes, 0)
        num_channels = (torch.size(x[0]))[1]
        output_size = (self.output_size)[0]
        dtype = ops.prim.dtype(x[0])
        device = ops.prim.device(x[0])
        _22 = [num_boxes, num_channels, output_size, output_size]
        output = torch.zeros(_22, dtype=dtype, layout=None, device=device, pin_memory=None)
        _23 = self.level_poolers
        _24 = getattr(_23, "0")
        _25 = getattr(_23, "1")
        _26 = getattr(_23, "2")
        _27 = getattr(_23, "3")
        _28 = _4(torch.eq(level_assignments, 0), )
        inds = _28[0]
        _29 = annotate(List[Optional[Tensor]], [inds])
        pooler_fmt_boxes_level = torch.index(pooler_fmt_boxes, _29)
        _30 = (_24).forward(x[0], pooler_fmt_boxes_level, )
        _31 = annotate(List[Optional[Tensor]], [inds])
        _32 = torch.index_put_(output, _31, _30, False)
        _33 = _4(torch.eq(level_assignments, 1), )
        inds0 = _33[0]
        _34 = annotate(List[Optional[Tensor]], [inds0])
        pooler_fmt_boxes_level0 = torch.index(pooler_fmt_boxes, _34)
        _35 = (_25).forward(x[1], pooler_fmt_boxes_level0, )
        _36 = annotate(List[Optional[Tensor]], [inds0])
        _37 = torch.index_put_(output, _36, _35, False)
        _38 = _4(torch.eq(level_assignments, 2), )
        inds1 = _38[0]
        _39 = annotate(List[Optional[Tensor]], [inds1])
        pooler_fmt_boxes_level1 = torch.index(pooler_fmt_boxes, _39)
        _40 = (_26).forward(x[2], pooler_fmt_boxes_level1, )
        _41 = annotate(List[Optional[Tensor]], [inds1])
        _42 = torch.index_put_(output, _41, _40, False)
        _43 = _4(torch.eq(level_assignments, 3), )
        inds2 = _43[0]
        _44 = annotate(List[Optional[Tensor]], [inds2])
        pooler_fmt_boxes_level2 = torch.index(pooler_fmt_boxes, _44)
        _45 = (_27).forward(x[3], pooler_fmt_boxes_level2, )
        _46 = annotate(List[Optional[Tensor]], [inds2])
        _47 = torch.index_put_(output, _46, _45, False)
        _19 = output
      _9 = _19
    return _9

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.layers.roi_align.ROIAlign
  __annotations__["1"] = __torch__.detectron2.layers.roi_align.ROIAlign
  __annotations__["2"] = __torch__.detectron2.layers.roi_align.ROIAlign
  __annotations__["3"] = __torch__.detectron2.layers.roi_align.ROIAlign
  training : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_38.ModuleList) -> None:
    _0 = uninitialized(None)
    ops.prim.RaiseException("")
    return _0
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_38.ModuleList) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.0, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0), False, False, None)
    _3 = self.output_size
    _4 = self.spatial_scale
    _5 = self.sampling_ratio
    _6 = self.aligned
    _7, _8, = _3
    _9 = _0(input0, _2, [_7, _8], _4, _5, _6, )
    return _9

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.1, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0), False, False, None)
    _3 = self.output_size
    _4 = self.spatial_scale
    _5 = self.sampling_ratio
    _6 = self.aligned
    _7, _8, = _3
    _9 = _0(input0, _2, [_7, _8], _4, _5, _6, )
    return _9

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.2, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0), False, False, None)
    _3 = self.output_size
    _4 = self.spatial_scale
    _5 = self.sampling_ratio
    _6 = self.aligned
    _7, _8, = _3
    _9 = _0(input0, _2, [_7, _8], _4, _5, _6, )
    return _9

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.3, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0), False, False, None)
    _3 = self.output_size
    _4 = self.spatial_scale
    _5 = self.sampling_ratio
    _6 = self.aligned
    _7, _8, = _3
    _9 = _0(input0, _2, [_7, _8], _4, _5, _6, )
    return _9

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head, type=FastRCNNConvFCHead:
class FastRCNNConvFCHead(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  _output_size : int
  flatten : __torch__.torch.nn.modules.flatten.Flatten
  fc1 : __torch__.torch.nn.modules.linear.Linear
  fc_relu1 : __torch__.torch.nn.modules.activation.ReLU
  fc2 : __torch__.torch.nn.modules.linear.___torch_mangle_39.Linear
  fc_relu2 : __torch__.torch.nn.modules.activation.ReLU
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.roi_heads.box_head.FastRCNNConvFCHead,
    x: Tensor) -> Tensor:
    _0 = self.flatten
    _1 = self.fc1
    _2 = self.fc_relu1
    _3 = self.fc2
    _4 = self.fc_relu2
    x0 = (_0).forward(x, )
    x1 = (_1).forward(x0, )
    x2 = (_2).forward(x1, )
    x3 = (_3).forward(x2, )
    return (_4).forward(x3, )
  def __len__(self: __torch__.detectron2.modeling.roi_heads.box_head.FastRCNNConvFCHead) -> int:
    return 5

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.flatten, type=Flatten:
class Flatten(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : None
  end_dim : Final[int] = -1
  start_dim : Final[int] = 1
  def forward(self: __torch__.torch.nn.modules.flatten.Flatten,
    input: Tensor) -> Tensor:
    return torch.flatten(input, 1, -1)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc1, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : None
  out_features : Final[int] = 1024
  in_features : Final[int] = 12544
  def forward(self: __torch__.torch.nn.modules.linear.Linear,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.linear
    return _0(input, self.weight, self.bias, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc_relu1, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : None
  inplace : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.relu(input, False, )
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc2, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : None
  out_features : Final[int] = 1024
  in_features : Final[int] = 1024
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_39.Linear,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.linear
    return _0(input, self.weight, self.bias, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc_relu2, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : None
  inplace : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.relu(input, False, )
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor, type=FastRCNNOutputLayers:
class FastRCNNOutputLayers(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  num_classes : int
  box2box_transform : __torch__.detectron2.modeling.box_regression.Box2BoxTransform
  smooth_l1_beta : float
  test_score_thresh : float
  test_nms_thresh : float
  test_topk_per_image : int
  box_reg_loss_type : str
  loss_weight : Dict[str, float]
  cls_score : __torch__.torch.nn.modules.linear.___torch_mangle_40.Linear
  bbox_pred : __torch__.torch.nn.modules.linear.___torch_mangle_41.Linear
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers,
    x: Tensor) -> Tuple[Tensor, Tensor]:
    if torch.gt(torch.dim(x), 2):
      x0 = torch.flatten(x, 1, -1)
    else:
      x0 = x
    scores = (self.cls_score).forward(x0, )
    proposal_deltas = (self.bbox_pred).forward(x0, )
    return (scores, proposal_deltas)
  def inference(self: __torch__.detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers,
    predictions: Tuple[Tensor, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> Tuple[List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1], List[Tensor]]:
    _0 = __torch__.detectron2.modeling.roi_heads.fast_rcnn.fast_rcnn_inference
    boxes = (self).predict_boxes(predictions, proposals, )
    scores = (self).predict_probs(predictions, proposals, )
    image_shapes = annotate(List[Tuple[int, int]], [])
    for _1 in range(torch.len(proposals)):
      x = proposals[_1]
      _2 = torch.append(image_shapes, x.image_size)
    _3 = _0(boxes, scores, image_shapes, self.test_score_thresh, self.test_nms_thresh, self.test_topk_per_image, )
    return _3
  def predict_boxes(self: __torch__.detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers,
    predictions: Tuple[Tensor, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> List[Tensor]:
    _4 = __torch__.detectron2.layers.wrappers.cat
    _5 = torch.__not__(bool(torch.len(proposals)))
    if _5:
      _6 = annotate(List[Tensor], [])
    else:
      _7, proposal_deltas, = predictions
      num_prop_per_image = annotate(List[int], [])
      for _8 in range(torch.len(proposals)):
        p = proposals[_8]
        _9 = torch.append(num_prop_per_image, (p).__len__())
      _10 = annotate(List[Tensor], [])
      for _11 in range(torch.len(proposals)):
        p0 = proposals[_11]
        _12 = (p0).__proposal_boxes_getter().tensor
        _13 = torch.append(_10, _12)
      proposal_boxes = _4(_10, 0, )
      predict_boxes = (self.box2box_transform).apply_deltas(proposal_deltas, proposal_boxes, )
      _14 = torch.split(predict_boxes, num_prop_per_image, 0)
      _6 = _14
    return _6
  def predict_probs(self: __torch__.detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers,
    predictions: Tuple[Tensor, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch1.ScriptedInstances1]) -> List[Tensor]:
    _15 = __torch__.torch.nn.functional.softmax
    scores, _16, = predictions
    num_inst_per_image = annotate(List[int], [])
    for _17 in range(torch.len(proposals)):
      p = proposals[_17]
      _18 = torch.append(num_inst_per_image, (p).__len__())
    probs = _15(scores, -1, 3, None, )
    _19 = torch.split(probs, num_inst_per_image, 0)
    return _19

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor.cls_score, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : None
  out_features : Final[int] = 2
  in_features : Final[int] = 1024
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_40.Linear,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.linear
    return _0(input, self.weight, self.bias, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor.bbox_pred, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : None
  out_features : Final[int] = 4
  in_features : Final[int] = 1024
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_41.Linear,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.linear
    return _0(input, self.weight, self.bias, )

--------------------------------------------------------------------------------